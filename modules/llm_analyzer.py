"""
Local LLM-Based Grease Analysis Module (FTIR Analysis with Peak Detection)

This module provides AI-powered FTIR analysis of grease spectroscopy data using
a local Ollama LLaVA model. Key features:

OPTIMIZATION FEATURES:
- Uses quantized LLaVA 7B model (llava:7b-v1.6-q4_K_M) for fast analysis
- Parallel processing with ThreadPoolExecutor (3 workers default)
- Peak detection from CSV coordinates (not graph visualization)
- Detailed FTIR report generation (2000 tokens)

ANALYSIS CAPABILITIES:
- Peak identification and comparison from CSV data
- Oxidation zone analysis (1650-1800 cm‚Åª¬π)
- Carbonyl peak detection (1725 cm‚Åª¬π)
- Structured FTIR report generation matching example format
- Batch parallel analysis for multiple samples
- Maintenance recommendations

PERFORMANCE:
- Single sample: ~5-15 seconds (depending on model)
- 5 samples (parallel): ~20-40 seconds
- Speedup: 2.5-3.5x with parallel processing

The module analyzes CSV coordinates directly to identify peaks and generate
detailed FTIR reports. Falls back to statistical analysis if Ollama is unavailable.
"""

import sys
import os

# Add project root to path for imports
if __name__ == "__main__":
    sys.path.insert(0, os.path.abspath(os.path.join(os.path.dirname(__file__), '..')))

from utils.config import LLM_CONFIG
from typing import Optional, Dict, List
import time
from concurrent.futures import ThreadPoolExecutor, as_completed
import pandas as pd
from modules.csv_processor import CSVProcessor


class LLMAnalyzer:
    """
    Local LLM Analysis Class with Parallel Processing Support
    
    Manages connection to Ollama LLM server and provides methods
    for analyzing grease spectroscopy data. Supports both single
    and batch analysis with parallel processing for efficiency.
    """
    
    def __init__(self, model: str = None, max_workers: int = None):
        """
        Initialize LLM Analyzer
        
        Args:
            model: Ollama model name (default: llama3.2:1b from config)
                  Smaller models are faster, larger are more detailed
            max_workers: Number of parallel threads for batch analysis
                        (default: 3 from config)
                        More workers = faster batch processing
        """
        self.model = model or LLM_CONFIG['model']
        self.timeout = LLM_CONFIG['timeout']
        self.max_workers = max_workers or LLM_CONFIG.get('max_workers', 3)
        self.ollama_available = self._check_ollama()
        
        if self.ollama_available:
            print(f"‚úÖ Ollama connected (Model: {self.model}, Workers: {self.max_workers})")
    
    def _check_ollama(self) -> bool:
        """
        Check Ollama Availability
        
        Tests if Ollama server is running and accessible.
        Attempts to list models and verify the configured model is available.
        
        Returns:
            True if Ollama is available, False otherwise
        """
        try:
            import ollama
            # Simple connectivity test
            models = ollama.list()
            
            # Check if the configured model is available
            model_names = [model.get('name', '') for model in models.get('models', [])]
            if self.model not in model_names:
                print(f"‚ö†Ô∏è Ollama is running, but model '{self.model}' is not installed.")
                print(f"   Available models: {', '.join(model_names[:3])}..." if model_names else "   No models found.")
                print(f"   Please install with: ollama pull {self.model}")
                return False
            
            return True
        except ConnectionError as e:
            print(f"‚ö†Ô∏è Ollama connection failed: Connection refused")
            print(f"   Please start Ollama service: ollama serve")
            return False
        except Exception as e:
            print(f"‚ö†Ô∏è Ollama connection failed: {str(e)}")
            print(f"   Make sure Ollama is installed and running: https://ollama.ai")
            return False
    
    def analyze_sample(self,
                      baseline_df: pd.DataFrame = None,
                      sample_df: pd.DataFrame = None,
                      baseline_stats: Dict = None,
                      sample_stats: Dict = None,
                      comparison: Dict = None,
                      baseline_name: str = "",
                      sample_name: str = "") -> str:
        """
        Analyze Single Sample Against Baseline
        
        Generates AI-powered FTIR analysis comparing a sample to baseline,
        analyzing peak positions, oxidation zones, and providing detailed assessment.
        
        Process:
        1. Extracts peak information from CSV coordinates (ALWAYS, even if Ollama unavailable)
        2. Analyzes oxidation zones (1650-1800 cm‚Åª¬π)
        3. Attempts to use Ollama LLM for analysis
        4. Falls back to structured FTIR report using peak data if Ollama unavailable
        
        Args:
            baseline_df: Baseline DataFrame with 'X' (wavenumber) and 'Y' (absorbance) columns
            sample_df: Sample DataFrame with 'X' (wavenumber) and 'Y' (absorbance) columns
            baseline_stats: Dictionary of baseline statistics (fallback if DataFrames not provided)
            sample_stats: Dictionary of sample statistics (fallback if DataFrames not provided)
            comparison: Comparison metrics
            baseline_name: Name of baseline file for context
            sample_name: Name of sample file for context
            
        Returns:
            Multi-line string with structured FTIR analysis report
        """
        # ALWAYS extract peak data from CSV coordinates (even if Ollama unavailable)
        peak_data = {}
        if baseline_df is not None and sample_df is not None:
            try:
                peak_data = self._extract_peak_data(baseline_df, sample_df)
            except Exception as e:
                print(f"‚ö†Ô∏è Peak extraction error: {str(e)}")
                peak_data = {}
        
        # Try to reconnect to Ollama (it might have started since initialization)
        ollama_available = self._check_ollama()
        if not ollama_available and not self.ollama_available:
            # Ollama is still not available - use structured fallback with peak data
            print("‚ö†Ô∏è Ollama unavailable - using structured fallback analysis with peak detection")
            return self._generate_structured_fallback_report(
                baseline_df, sample_df,
                baseline_stats, sample_stats, comparison,
                peak_data,
                baseline_name, sample_name
            )
        
        # Update availability status
        if ollama_available:
            self.ollama_available = True
        
        # Try to use Ollama LLM
        try:
            import ollama
            
            # Generate detailed FTIR report prompt
            prompt = self._create_ftir_prompt(
                baseline_df, sample_df,
                baseline_stats, sample_stats, comparison,
                peak_data,
                baseline_name, sample_name
            )
            
            # Call LLM with optimized settings
            start_time = time.time()
            response = ollama.chat(
                model=self.model,
                messages=[
                    {
                        'role': 'system',
                        'content': 'You are an expert FTIR spectroscopy analyst specializing in grease analysis. Generate detailed, structured reports analyzing peak positions, oxidation zones, and grease condition assessment.'
                    },
                    {
                        'role': 'user',
                        'content': prompt
                    }
                ],
                options={
                    'temperature': LLM_CONFIG['temperature'],
                    'num_predict': LLM_CONFIG.get('num_predict', 2000),
                    'num_ctx': LLM_CONFIG.get('num_ctx', 4096),
                }
            )
            
            elapsed_time = time.time() - start_time
            print(f"‚è±Ô∏è {sample_name} LLM response time: {elapsed_time:.2f}Seconds")
            
            return response['message']['content']
            
        except Exception as e:
            print(f"‚ö†Ô∏è LLM analysis failed: {str(e)}")
            print("‚ö†Ô∏è Falling back to structured analysis using peak detection")
            # Fall back to structured report using peak data
            return self._generate_structured_fallback_report(
                baseline_df, sample_df,
                baseline_stats, sample_stats, comparison,
                peak_data,
                baseline_name, sample_name
            )
    
    def _extract_peak_data(self, baseline_df: pd.DataFrame, sample_df: pd.DataFrame) -> Dict:
        """
        Extract Peak and Region Data from DataFrames
        
        Analyzes CSV coordinates to identify:
        - Major peaks in baseline and sample
        - Oxidation zone analysis (1650-1800 cm‚Åª¬π)
        - Key regions of interest
        
        Args:
            baseline_df: Baseline DataFrame
            sample_df: Sample DataFrame
            
        Returns:
            Dictionary containing peak and region analysis data
        """
        peak_data = {}
        
        try:
            # Detect all peaks (sorted by wavenumber, not absorbance)
            baseline_peaks = CSVProcessor.detect_peaks(baseline_df, min_distance=20)
            sample_peaks = CSVProcessor.detect_peaks(sample_df, min_distance=20)
            
            # Identify significant peaks in key regions (keep sorted by wavenumber)
            # This ensures we capture peaks in all important regions, not just highest absorbance
            significant_baseline = CSVProcessor.get_significant_peaks_by_region(baseline_peaks, baseline_df)
            significant_sample = CSVProcessor.get_significant_peaks_by_region(sample_peaks, sample_df)
            
            peak_data['baseline_peaks'] = significant_baseline
            peak_data['sample_peaks'] = significant_sample
            peak_data['baseline_peaks_all'] = baseline_peaks  # Keep all for matching
            peak_data['sample_peaks_all'] = sample_peaks  # Keep all for matching
            
            # Analyze oxidation zone (1650-1800 cm‚Åª¬π) - critical for grease health
            oxidation_zone_baseline = CSVProcessor.get_region_statistics(baseline_df, 1650, 1800)
            oxidation_zone_sample = CSVProcessor.get_region_statistics(sample_df, 1650, 1800)
            
            peak_data['oxidation_zone_baseline'] = oxidation_zone_baseline
            peak_data['oxidation_zone_sample'] = oxidation_zone_sample
            
            # Get value at typical carbonyl peak (1725 cm‚Åª¬π)
            carbonyl_baseline = CSVProcessor.get_value_at_wavenumber(baseline_df, 1725, tolerance=10)
            carbonyl_sample = CSVProcessor.get_value_at_wavenumber(sample_df, 1725, tolerance=10)
            
            peak_data['carbonyl_baseline'] = carbonyl_baseline
            peak_data['carbonyl_sample'] = carbonyl_sample
            
            # Additional key regions
            # C-H stretch region (2800-3000 cm‚Åª¬π)
            ch_zone_baseline = CSVProcessor.get_region_statistics(baseline_df, 2800, 3000)
            ch_zone_sample = CSVProcessor.get_region_statistics(sample_df, 2800, 3000)
            
            peak_data['ch_zone_baseline'] = ch_zone_baseline
            peak_data['ch_zone_sample'] = ch_zone_sample
            
            # O-H region (3200-3600 cm‚Åª¬π)
            oh_zone_baseline = CSVProcessor.get_region_statistics(baseline_df, 3200, 3600)
            oh_zone_sample = CSVProcessor.get_region_statistics(sample_df, 3200, 3600)
            
            peak_data['oh_zone_baseline'] = oh_zone_baseline
            peak_data['oh_zone_sample'] = oh_zone_sample
            
        except Exception as e:
            print(f"‚ö†Ô∏è Peak extraction error: {str(e)}")
            peak_data = {}
        
        return peak_data
    
    def _create_ftir_prompt(self,
                           baseline_df: pd.DataFrame,
                           sample_df: pd.DataFrame,
                           baseline_stats: Dict,
                           sample_stats: Dict,
                           comparison: Dict,
                           peak_data: Dict,
                           baseline_name: str,
                           sample_name: str) -> str:
        """
        Create Detailed FTIR Analysis Prompt
        
        Generates a comprehensive prompt for structured FTIR grease analysis report.
        Includes peak positions, oxidation analysis, and maintenance recommendations.
        
        Returns:
            Formatted prompt string for LLM
        """
        # Extract sample ID from filename (remove .csv extension)
        sample_id = sample_name.replace('.csv', '').strip()
        
        prompt = f"""Generate a detailed FTIR Grease Analysis Report for the following comparison:

======================================================================
SAMPLE: {sample_id}
BASELINE: {baseline_name}
======================================================================

PEAK DATA (from CSV coordinates):
"""
        
        # Add major peaks information (sorted by wavenumber, not absorbance)
        if peak_data.get('baseline_peaks') and peak_data.get('sample_peaks'):
            prompt += "\n**MAJOR PEAKS (Key Regions - Sorted by Wavenumber):**\n\n"
            
            baseline_peaks = peak_data['baseline_peaks']
            sample_peaks = peak_data['sample_peaks']
            
            prompt += "Baseline Peaks (by region):\n"
            for i, bp in enumerate(baseline_peaks[:5], 1):
                prompt += f"  Peak {i}: ~{bp['wavenumber']:.0f} cm‚Åª¬π, Absorbance: ~{bp['absorbance']:.2f} A\n"
            
            prompt += "\nSample Peaks (by region):\n"
            for i, sp in enumerate(sample_peaks[:5], 1):
                prompt += f"  Peak {i}: ~{sp['wavenumber']:.0f} cm‚Åª¬π, Absorbance: ~{sp['absorbance']:.2f} A\n"
            
            # Find matching peaks (within 50 cm‚Åª¬π) - match by proximity
            prompt += "\n**PEAK COMPARISON (Matched by Wavenumber Proximity):**\n"
            matched_pairs = []
            used_sample_indices = set()
            
            for bp in baseline_peaks[:5]:
                best_match = None
                best_distance = float('inf')
                best_idx = -1
                
                for idx, sp in enumerate(sample_peaks):
                    if idx in used_sample_indices:
                        continue
                    distance = abs(sp['wavenumber'] - bp['wavenumber'])
                    if distance < best_distance and distance < 50:
                        best_distance = distance
                        best_match = sp
                        best_idx = idx
                
                if best_match:
                    matched_pairs.append((bp, best_match))
                    used_sample_indices.add(best_idx)
                    change = "Increased" if best_match['absorbance'] > bp['absorbance'] * 1.1 else \
                             "Decreased" if best_match['absorbance'] < bp['absorbance'] * 0.9 else "Similar"
                    shift_note = f" (shifted {best_match['wavenumber'] - bp['wavenumber']:+.0f} cm‚Åª¬π)" if abs(best_match['wavenumber'] - bp['wavenumber']) > 5 else ""
                    prompt += f"  Peak ~{bp['wavenumber']:.0f} cm‚Åª¬π: Baseline={bp['absorbance']:.2f} A, Sample={best_match['absorbance']:.2f} A ({change}{shift_note})\n"
        
        # Add oxidation zone analysis
        if 'oxidation_zone_baseline' in peak_data and 'oxidation_zone_sample' in peak_data:
            ox_base = peak_data['oxidation_zone_baseline']
            ox_sample = peak_data['oxidation_zone_sample']
            
            prompt += f"""

**OXIDATION ZONE ANALYSIS (1650-1800 cm‚Åª¬π - CRITICAL FOR GREASE HEALTH):**
- Baseline max absorbance: {ox_base['max']:.2f} A at {ox_base['max_wavenumber']:.0f} cm‚Åª¬π
- Sample max absorbance: {ox_sample['max']:.2f} A at {ox_sample['max_wavenumber']:.0f} cm‚Åª¬π
- Change: {"Increased" if ox_sample['max'] > ox_base['max'] else "Decreased" if ox_sample['max'] < ox_base['max'] else "Similar"}

**Carbonyl Peak (1725 cm‚Åª¬π):**
- Baseline: {peak_data.get('carbonyl_baseline', 'N/A')} A
- Sample: {peak_data.get('carbonyl_sample', 'N/A')} A
"""
        
        # Add statistical comparison
        if comparison:
            prompt += f"""

**STATISTICAL COMPARISON:**
- Mean deviation: {comparison.get('mean_deviation_percent', 0):+.1f}%
- Std deviation: {comparison.get('std_deviation_percent', 0):+.1f}%
- Correlation: {comparison.get('correlation', 0):.3f}
- Quality score: {comparison.get('quality_score', 0):.1f}/100
"""
        
        prompt += f"""

======================================================================
INSTRUCTIONS:
======================================================================

Generate a structured FTIR Analysis Report following this EXACT format:

======================================================================
AI ANALYSIS REPORT - FTIR GREASE COMPARISON
======================================================================

======================================================================
SAMPLE 1: {sample_id}
======================================================================

1. MAJOR PEAKS IDENTIFICATION
-------------------------------------------------------------------
[Identify and list 3-5 major peaks. For each peak:
- Peak X: Around [wavenumber] cm‚Åª¬π
- Baseline absorbance: ~[value] A
- Sample absorbance: ~[value] A
- Change: [Similar/Increased/Decreased]]

2. OXIDATION ANALYSIS (CRITICAL FOR GREASE HEALTH)
-------------------------------------------------------------------
‚ö†Ô∏è FOCUS on the region 1650-1800 cm‚Åª¬π (carbonyl zone):
- Does the sample show a distinct peak around 1700-1750 cm‚Åª¬π?
[YES/NO]
- If YES:
  - Approximate wavenumber: [value] cm‚Åª¬π
  - Approximate absorbance value: ~[value] A
  - Is this peak HIGHER than the baseline in this region? [Yes/No]
- Oxidation Assessment:
[LOW/MODERATE/HIGH/CRITICAL]

3. KEY DIFFERENCES BETWEEN BASELINE AND SAMPLE
-------------------------------------------------------------------
[Analyze and report:
- New peaks in sample (not present in baseline)
- Peak shifts
- Peak broadening
- Intensity changes]

4. GREASE CONDITION ASSESSMENT
-------------------------------------------------------------------
**Overall Grease Health:**
[EXCELLENT/GOOD/FAIR/POOR]

**Oxidation Status:**
[LOW/MODERATE/HIGH/CRITICAL]

**Contamination Indicators:**
[Describe any contamination indicators or state "No clear indications"]

5. MAINTENANCE RECOMMENDATION
-------------------------------------------------------------------
‚ö†Ô∏è DECISION FOR EQUIPMENT MANAGER:

**Action Required:**
[NO ACTION/MONITOR CLOSELY/SCHEDULE INSPECTION/IMMEDIATE ACTION]

**Reasoning:**
[Provide 2-3 sentence explanation based on the analysis]

**Re-test Interval:**
[2 weeks/4 weeks/8 weeks/Immediate]

======================================================================

Generate the report now:"""
        
        return prompt
    
    def _create_prompt(self,
                      baseline_stats: Dict,
                      sample_stats: Dict,
                      comparison: Dict,
                      baseline_name: str,
                      sample_name: str) -> str:
        """
        Create Optimized Analysis Prompt (Legacy/Concise Version)
        
        Generates a structured prompt for the LLM that includes:
        - Baseline and sample statistics
        - Deviation percentages
        - Correlation and quality score
        - Specific analysis requests
        
        Returns:
            Formatted prompt string for LLM
        """
        
        prompt = f"""Grease Analysis: {sample_name} vs {baseline_name}

**Baseline:** Mean={baseline_stats['mean']:.1f}, Std={baseline_stats['std']:.1f}, N={baseline_stats['count']}
**Sample:** Mean={sample_stats['mean']:.1f}, Std={sample_stats['std']:.1f}, N={sample_stats['count']}

**Deviations:**
‚Ä¢ Mean: {comparison['mean_deviation_percent']:+.1f}%
‚Ä¢ Std: {comparison['std_deviation_percent']:+.1f}%
‚Ä¢ Correlation: {comparison['correlation']:.2f}
‚Ä¢ Quality: {comparison['quality_score']:.0f}/100

Provide 4 concise bullet points:
1. Key deviation from baseline
2. Pattern change assessment
3. Overall condition
4. Action needed (if any)"""
        
        return prompt
    
    def _generate_structured_fallback_report(self,
                                            baseline_df: pd.DataFrame,
                                            sample_df: pd.DataFrame,
                                            baseline_stats: Dict,
                                            sample_stats: Dict,
                                            comparison: Dict,
                                            peak_data: Dict,
                                            baseline_name: str,
                                            sample_name: str) -> str:
        """
        Generate Structured FTIR Report Using Peak Data (Fallback when Ollama unavailable)
        
        Creates a structured FTIR report format matching the example, using:
        - Peak detection from CSV coordinates
        - Oxidation zone analysis
        - Rule-based assessments
        
        Returns:
            Structured FTIR analysis report in the exact format requested
        """
        sample_id = sample_name.replace('.csv', '').strip()
        report = []
        
        report.append("=" * 70)
        report.append("AI ANALYSIS REPORT - FTIR GREASE COMPARISON")
        report.append("=" * 70)
        report.append("")
        report.append("=" * 70)
        report.append(f"SAMPLE 1: {sample_id}")
        report.append("=" * 70)
        report.append("")
        
        # 1. MAJOR PEAKS IDENTIFICATION
        report.append("1. MAJOR PEAKS IDENTIFICATION")
        report.append("-" * 70)
        report.append("")
        
        if peak_data.get('baseline_peaks') and peak_data.get('sample_peaks'):
            # Match peaks by wavenumber proximity (not just zipping)
            baseline_peaks = peak_data['baseline_peaks']
            sample_peaks = peak_data['sample_peaks']
            
            # Get up to 5 most significant peaks to report
            peaks_to_report = min(5, max(len(baseline_peaks), len(sample_peaks)))
            
            matched_pairs = []
            used_sample_indices = set()
            
            # Match each baseline peak to closest sample peak
            for bp in baseline_peaks[:peaks_to_report]:
                best_match = None
                best_distance = float('inf')
                best_idx = -1
                
                for idx, sp in enumerate(sample_peaks):
                    if idx in used_sample_indices:
                        continue
                    distance = abs(sp['wavenumber'] - bp['wavenumber'])
                    if distance < best_distance and distance < 50:  # Within 50 cm‚Åª¬π
                        best_distance = distance
                        best_match = sp
                        best_idx = idx
                
                if best_match:
                    matched_pairs.append((bp, best_match))
                    used_sample_indices.add(best_idx)
                else:
                    # No close match found, report baseline peak alone
                    matched_pairs.append((bp, None))
            
            # Report matched peaks
            for i, (bp, sp) in enumerate(matched_pairs[:5], 1):
                if sp is None:
                    report.append(f"- Peak {i}: Around {bp['wavenumber']:.0f} cm‚Åª¬π (approximate wavenumber)")
                    report.append(f"  - Baseline absorbance: ~{bp['absorbance']:.2f} A")
                    report.append(f"  - Sample absorbance: ~N/A (peak not found in sample)")
                    report.append(f"  - Change: [Not Present in Sample]")
                else:
                    # Determine change
                    wavenumber_diff = abs(sp['wavenumber'] - bp['wavenumber'])
                    abs_diff = abs(sp['absorbance'] - bp['absorbance'])
                    abs_change_pct = (abs_diff / max(bp['absorbance'], 0.001)) * 100
                    
                    if wavenumber_diff > 10:
                        change = "Shifted"
                    elif sp['absorbance'] > bp['absorbance'] * 1.1:
                        change = "Increased"
                    elif sp['absorbance'] < bp['absorbance'] * 0.9:
                        change = "Decreased"
                    else:
                        change = "Similar"
                    
                    # Use the baseline wavenumber as reference
                    report.append(f"- Peak {i}: Around {bp['wavenumber']:.0f} cm‚Åª¬π (approximate wavenumber)")
                    report.append(f"  - Baseline absorbance: ~{bp['absorbance']:.2f} A")
                    report.append(f"  - Sample absorbance: ~{sp['absorbance']:.2f} A")
                    report.append(f"  - Change: [{change}]")
                report.append("")
        else:
            report.append("- Peak detection unavailable (insufficient data or processing error)")
            report.append("")
        
        # 2. OXIDATION ANALYSIS
        report.append("2. OXIDATION ANALYSIS (CRITICAL FOR GREASE HEALTH)")
        report.append("-" * 70)
        report.append("")
        report.append("‚ö†Ô∏è FOCUS on the region 1650-1800 cm‚Åª¬π (carbonyl zone):")
        report.append("")
        
        if 'oxidation_zone_baseline' in peak_data and 'oxidation_zone_sample' in peak_data:
            ox_base = peak_data['oxidation_zone_baseline']
            ox_sample = peak_data['oxidation_zone_sample']
            
            # Check if there's a distinct peak in carbonyl region
            has_carbonyl_peak = ox_sample['max'] > ox_base['max'] * 1.1 or ox_sample['max'] > 0.3
            carbonyl_peak_higher = ox_sample['max'] > ox_base['max']
            
            report.append(f"- Does the BLUE line show a distinct peak around 1700-1750 cm‚Åª¬π?")
            report.append(f"  [{'YES' if has_carbonyl_peak else 'NO'}]")
            report.append("")
            
            if has_carbonyl_peak:
                carbonyl_val = peak_data.get('carbonyl_sample') or ox_sample['max_wavenumber']
                report.append(f"- If YES:")
                report.append(f"  - Approximate wavenumber: {ox_sample['max_wavenumber']:.0f} cm‚Åª¬π")
                report.append(f"  - Approximate absorbance value: ~{ox_sample['max']:.2f} A")
                report.append(f"  - Is this peak HIGHER than the baseline in this region? [{'Yes' if carbonyl_peak_higher else 'No'}]")
                report.append("")
            
            # Determine oxidation level
            ox_increase = ((ox_sample['max'] - ox_base['max']) / max(ox_base['max'], 0.001)) * 100
            if ox_increase > 50:
                ox_assessment = "HIGH"
            elif ox_increase > 20:
                ox_assessment = "MODERATE"
            elif ox_increase > 5:
                ox_assessment = "LOW"
            else:
                ox_assessment = "LOW"
            
            report.append(f"- Oxidation Assessment:")
            report.append(f"  [{ox_assessment}]")
            report.append("")
        else:
            report.append("- Oxidation zone analysis unavailable")
            report.append("")
        
        # 3. KEY DIFFERENCES
        report.append("3. KEY DIFFERENCES BETWEEN BASELINE AND SAMPLE")
        report.append("-" * 70)
        report.append("")
        
        differences = []
        if peak_data.get('baseline_peaks_all') and peak_data.get('sample_peaks_all'):
            # Use all peaks for better matching
            baseline_all = peak_data['baseline_peaks_all']
            sample_all = peak_data['sample_peaks_all']
            
            # Check for new peaks in sample (not within 30 cm‚Åª¬π of any baseline peak)
            baseline_wavenumbers = [p['wavenumber'] for p in baseline_all]
            new_peaks = []
            for sp in sample_all:
                closest_baseline_dist = min([abs(bw - sp['wavenumber']) for bw in baseline_wavenumbers])
                if closest_baseline_dist > 30:  # New peak if >30 cm‚Åª¬π away
                    new_peaks.append(sp)
            
            if new_peaks:
                # Get the most significant new peak
                significant_new = max(new_peaks, key=lambda x: x['absorbance'])
                if significant_new['absorbance'] > 0.5:  # Only report if significant
                    differences.append(f"‚úì New peaks in sample (not present in baseline):")
                    differences.append(f"  - Location: {significant_new['wavenumber']:.0f} cm‚Åª¬π, Absorbance: ~{significant_new['absorbance']:.2f} A")
            
            # Check for peak shifts using matched pairs
            if peak_data.get('baseline_peaks') and peak_data.get('sample_peaks'):
                for bp in peak_data['baseline_peaks'][:5]:
                    # Find closest sample peak
                    closest_sample = min(sample_all, 
                                        key=lambda sp: abs(sp['wavenumber'] - bp['wavenumber']))
                    if abs(closest_sample['wavenumber'] - bp['wavenumber']) > 10:
                        differences.append(f"‚úì Peak shifts:")
                        differences.append(f"  - Baseline peak at {bp['wavenumber']:.0f} cm‚Åª¬π shifted to {closest_sample['wavenumber']:.0f} cm‚Åª¬π in sample")
                        break
            
            # Check intensity changes
            overall_increase = abs(comparison.get('mean_deviation_percent', 0)) > 5
            if overall_increase:
                differences.append(f"‚úì Intensity changes:")
                differences.append(f"  - Overall absorbance: [{'Higher' if comparison.get('mean_deviation_percent', 0) > 0 else 'Lower'}] than baseline")
        
        if not differences:
            differences.append("  - No significant differences detected (minor variations within normal range)")
        
        report.extend(differences)
        report.append("")
        
        # 4. GREASE CONDITION ASSESSMENT
        report.append("4. GREASE CONDITION ASSESSMENT")
        report.append("-" * 70)
        report.append("")
        
        quality = comparison.get('quality_score', 0)
        corr = comparison.get('correlation', 0)
        
        if quality > 85:
            health = "EXCELLENT"
        elif quality > 70:
            health = "GOOD"
        elif quality > 50:
            health = "FAIR"
        else:
            health = "POOR"
        
        ox_assessment = "LOW"
        if 'oxidation_zone_baseline' in peak_data and 'oxidation_zone_sample' in peak_data:
            ox_base = peak_data['oxidation_zone_baseline']
            ox_sample = peak_data['oxidation_zone_sample']
            ox_increase = ((ox_sample['max'] - ox_base['max']) / max(ox_base['max'], 0.001)) * 100
            if ox_increase > 50:
                ox_assessment = "HIGH"
            elif ox_increase > 20:
                ox_assessment = "MODERATE"
        
        report.append("**Overall Grease Health:**")
        report.append(f"[{health}]")
        report.append("")
        report.append("**Oxidation Status:**")
        report.append(f"[{ox_assessment}]")
        report.append("")
        report.append("**Contamination Indicators:**")
        if corr < 0.85 or abs(comparison.get('std_deviation_percent', 0)) > 30:
            report.append("Potential contamination indicated by significant pattern changes or high variability.")
        else:
            report.append("No clear indications of contamination are present in the spectrum.")
        report.append("")
        
        # 5. MAINTENANCE RECOMMENDATION
        report.append("5. MAINTENANCE RECOMMENDATION")
        report.append("-" * 70)
        report.append("")
        report.append("‚ö†Ô∏è DECISION FOR EQUIPMENT MANAGER:")
        report.append("")
        
        if quality > 85:
            action = "NO ACTION"
            reasoning = "The grease sample shows excellent correlation with the baseline and minimal deviations. The sample is in good condition and no immediate action is required."
            retest = "8 weeks"
        elif quality > 70:
            action = "MONITOR CLOSELY"
            reasoning = f"The grease sample exhibits a quality score of {quality:.1f}/100 with {comparison.get('mean_deviation_percent', 0):+.1f}% mean deviation. While still in acceptable range, monitoring is recommended to track any trends."
            retest = "4 weeks"
        elif quality > 50:
            action = "SCHEDULE INSPECTION"
            reasoning = f"The grease sample shows moderate deviations (quality score: {quality:.1f}/100, correlation: {corr:.3f}). An inspection should be scheduled to assess the condition and determine if replacement is needed."
            retest = "2 weeks"
        else:
            action = "IMMEDIATE ACTION"
            reasoning = f"The grease sample shows significant deviations from baseline (quality score: {quality:.1f}/100). Immediate action is recommended to prevent equipment failure."
            retest = "Immediate"
        
        report.append("**Action Required:**")
        report.append(f"[{action}]")
        report.append("")
        report.append("**Reasoning:**")
        report.append(f"{reasoning}")
        report.append("")
        report.append("**Re-test Interval:**")
        report.append(f"[{retest}]")
        report.append("")
        
        # Add note about Ollama
        report.append("")
        report.append("=" * 70)
        report.append("NOTE: This analysis was generated using peak detection from CSV coordinates.")
        report.append("For enhanced AI analysis, please ensure Ollama is running with the LLaVA model.")
        report.append("=" * 70)
        
        return "\n".join(report)
    
    def _fallback_analysis(self,
                          baseline_stats: Dict,
                          sample_stats: Dict,
                          comparison: Dict) -> str:
        """
        Fallback Analysis (Legacy - Simple Rule-Based)
        
        Provides simple statistical analysis when LLM is unavailable and no peak data available.
        This is a legacy method - prefer _generate_structured_fallback_report instead.
        
        Returns:
            Formatted analysis text with bullet points
        """
        analysis = []
        
        # Analyze mean deviation
        mean_dev = comparison['mean_deviation_percent']
        if abs(mean_dev) < 5:
            analysis.append(f"‚úì Mean intensity deviation is minimal ({mean_dev:+.1f}%), indicating stable condition.")
        elif abs(mean_dev) < 15:
            analysis.append(f"‚ö† Moderate mean deviation detected ({mean_dev:+.1f}%), monitor for trends.")
        else:
            analysis.append(f"‚ö†Ô∏è Significant mean deviation ({mean_dev:+.1f}%), investigation recommended.")
        
        # Analyze standard deviation changes
        std_dev = comparison['std_deviation_percent']
        if abs(std_dev) > 20:
            analysis.append(f"‚ö†Ô∏è High variability change ({std_dev:+.1f}%), possible contamination or degradation.")
        else:
            analysis.append(f"‚úì Variability remains consistent ({std_dev:+.1f}%).")
        
        # Analyze correlation coefficient
        corr = comparison['correlation']
        if corr > 0.95:
            analysis.append(f"‚úì Excellent correlation with baseline ({corr:.3f}), pattern maintained.")
        elif corr > 0.85:
            analysis.append(f"‚ö† Good correlation ({corr:.3f}), minor pattern shift detected.")
        else:
            analysis.append(f"‚ö†Ô∏è Low correlation ({corr:.3f}), significant pattern change observed.")
        
        # Quality score analysis
        quality = comparison['quality_score']
        if quality > 85:
            analysis.append(f"‚úì Overall Quality: Excellent ({quality:.1f}/100) - No action needed.")
        elif quality > 70:
            analysis.append(f"‚ö† Overall Quality: Good ({quality:.1f}/100) - Continue monitoring.")
        elif quality > 50:
            analysis.append(f"‚ö†Ô∏è Overall Quality: Fair ({quality:.1f}/100) - Schedule inspection.")
        else:
            analysis.append(f"‚ùå Overall Quality: Poor ({quality:.1f}/100) - Immediate action required.")
        
        return "\n".join(f"‚Ä¢ {item}" for item in analysis)
    
    def generate_summary(self, all_analyses: Dict[str, str]) -> str:
        """
        Generate Summary for All Samples
        
        Args:
            all_analyses: Dictionary of {sample_name: analysis_result}
            
        Returns:
            Overall summary text
        """
        if not self.ollama_available:
            return self._fallback_summary(all_analyses)
        
        try:
            import ollama
            
            # Create concise summary prompt (truncate analyses for speed)
            analyses_text = "\n".join([
                f"{name}: {analysis[:150]}..."  # Limit each to 150 chars
                for name, analysis in list(all_analyses.items())[:5]  # Max 5 samples
            ])
            
            prompt = f"""Summary of {len(all_analyses)} grease samples:

{analyses_text}

Provide 3-sentence summary:
1. Overall condition
2. Samples needing attention
3. Key recommendation"""
            
            response = ollama.chat(
                model=self.model,
                messages=[
                    {
                        'role': 'system',
                        'content': 'Maintenance manager. Be brief.'
                    },
                    {
                        'role': 'user',
                        'content': prompt
                    }
                ],
                options={
                    'temperature': 0.3,
                    'num_predict': 150,  # Shorter for summaries
                }
            )
            
            return response['message']['content']
            
        except Exception as e:
            print(f"‚ö†Ô∏è Summary generation failed: {str(e)}")
            return self._fallback_summary(all_analyses)
    
    def _fallback_summary(self, all_analyses: Dict[str, str]) -> str:
        """
        Fallback Summary (Rule-Based)
        
        Provides simple statistical summary when LLM is unavailable.
        
        Returns:
            Basic summary text with sample count and quality threshold info
        """
        total = len(all_analyses)
        
        summary = f"Analyzed {total} sample(s) against baseline. "
        summary += "Review individual analyses above for detailed insights. "
        summary += "Samples with quality scores below 70 require attention."
        
        return summary
    
    def analyze_samples_batch(self, samples_data: List[Dict]) -> Dict[str, str]:
        """
        Analyze Multiple Samples in Parallel (3-5x Speedup)
        
        Uses ThreadPoolExecutor to analyze multiple samples simultaneously
        instead of sequentially. This is the key optimization that enables
        fast batch processing.
        
        Performance Example (5 samples):
        - Sequential: ~40-80 seconds (one at a time)
        - Parallel: ~15-25 seconds (3 simultaneous)
        - Speedup: 2.5-3.5x faster
        
        How It Works:
        1. Creates thread pool with max_workers threads (default: 3)
        2. Submits all analysis tasks to pool
        3. Tasks run in parallel on separate threads
        4. Collects results as they complete
        5. Returns all results together
        
        Args:
            samples_data: List of sample dictionaries, each containing:
                - 'baseline_df': Baseline DataFrame (preferred)
                - 'sample_df': Sample DataFrame (preferred)
                - 'baseline_stats': Baseline statistics (fallback)
                - 'sample_stats': Sample statistics (fallback)
                - 'comparison': Comparison metrics
                - 'baseline_name': Baseline filename
                - 'sample_name': Sample filename
            
        Returns:
            Dictionary mapping sample_name -> analysis_text
            Example: {'sample_01.csv': 'Analysis text...', ...}
        """
        results = {}
        total = len(samples_data)
        
        print(f"\nüöÄ Starting parallel analysis ({total} samples, {self.max_workers} workers)")
        start_time = time.time()
        
        with ThreadPoolExecutor(max_workers=self.max_workers) as executor:
            # Submit all analysis tasks simultaneously
            future_to_sample = {
                executor.submit(
                    self.analyze_sample,
                    data.get('baseline_df'),
                    data.get('sample_df'),
                    data.get('baseline_stats'),
                    data.get('sample_stats'),
                    data.get('comparison'),
                    data.get('baseline_name', ''),
                    data.get('sample_name', '')
                ): data['sample_name']
                for data in samples_data
            }
            
            # Collect results as they complete (not necessarily in order)
            completed = 0
            for future in as_completed(future_to_sample):
                sample_name = future_to_sample[future]
                try:
                    results[sample_name] = future.result()
                    completed += 1
                    print(f"  ‚úÖ [{completed}/{total}] {sample_name}")
                except Exception as e:
                    print(f"  ‚ùå {sample_name} analysis failed: {e}")
                    results[sample_name] = f"Analysis failed: {str(e)}"
        
        elapsed = time.time() - start_time
        avg_time = elapsed / total if total > 0 else 0
        print(f"\n‚úÖ Parallel analysis complete - Total {elapsed:.1f}s (avg {avg_time:.1f}s/sample)\n")
        
        return results


# ============================================================================
# TEST CODE - Run this file directly to verify functionality
# ============================================================================
if __name__ == "__main__":
    print("‚úÖ LLM Analyzer Test")
    
    analyzer = LLMAnalyzer()
    
    # Test data
    baseline_stats = {
        'mean': 100.0,
        'std': 10.0,
        'min': 80.0,
        'max': 120.0,
        'count': 300
    }
    
    sample_stats = {
        'mean': 110.0,
        'std': 12.0,
        'min': 85.0,
        'max': 135.0,
        'count': 300
    }
    
    comparison = {
        'mean_deviation_percent': 10.0,
        'std_deviation_percent': 20.0,
        'correlation': 0.92,
        'quality_score': 78.5
    }
    
    print("\nAnalyzing...")
    result = analyzer.analyze_sample(
        baseline_stats,
        sample_stats,
        comparison,
        "baseline.csv",
        "sample_01.csv"
    )
    
    print("\nAnalysis result:")
    print(result)
    print("\n‚úÖ Module operational")