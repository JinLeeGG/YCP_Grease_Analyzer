"""
Optimized Hybrid FTIR Analysis Module

This module uses a PRODUCTION-READY hybrid approach:
1. Primary Analysis: Fast numerical peak detection with FTIRAnalyzer (<1s)
2. Optional Enhancement: LLaVA vision model for natural language summaries (5-15s)
3. Automatic Fallback: Always returns structured results, even if LLM fails

KEY FEATURES:
- 10-50x faster than LLM-only approach
- Statistical rigor (3Ïƒ peak significance)
- Critical region monitoring (oxidation, water, glycol)
- Rule-based decision logic with confidence scores
- 100% reliable (no LLM dependency for core analysis)

PERFORMANCE:
- Core analysis: <1s (FTIRAnalyzer)
- With LLM enhancement: 5-15s (optional)
- Batch processing: Fully parallelizable
"""

import sys
import os

# Add project root to path for imports
if __name__ == "__main__":
    sys.path.insert(0, os.path.abspath(os.path.join(os.path.dirname(__file__), '..')))

from utils.config import LLM_CONFIG
from typing import Optional, Dict, List, Tuple
import time
import base64
from pathlib import Path
import pandas as pd
import numpy as np

# Import the optimized FTIR analyzer
from modules.ftir_peak_analyzer import (
    FTIRAnalyzer, 
    AnalysisConfig, 
    load_csv_spectrum_from_df,
    choose_baseline_auto
)



class LLMAnalyzer:
    """
    Optimized Hybrid FTIR Analyzer
    
    Architecture:
    1. FTIRAnalyzer (Primary): Fast numerical analysis (<1s, always works)
    2. LLaVA (Optional): Natural language enhancement (5-15s, if available)
    3. Automatic Fallback: Returns structured results regardless of LLM status
    
    Usage:
        analyzer = LLMAnalyzer()
        result = analyzer.analyze_sample(baseline_df, sample_df, ...)
        # Returns structured results in <1s, with optional LLM enhancement
    """
    
    def __init__(self, model: str = "llava:7b-v1.6", use_llm: bool = True):
        """
        Initialize Hybrid Analyzer
        
        Args:
            model: Ollama vision model name (default: llava:7b-v1.6 for speed)
            use_llm: Whether to attempt LLM enhancement (default: True)
        """
        self.model = model
        self.use_llm = use_llm
        self.ollama_available = self._check_ollama() if use_llm else False
        
        # Initialize core FTIR analyzer (always available)
        self.ftir_analyzer = FTIRAnalyzer(AnalysisConfig())
        
        if self.ollama_available:
            print(f"âœ… Hybrid Mode: FTIRAnalyzer + LLM ({self.model})")
        else:
            print(f"âœ… Fast Mode: FTIRAnalyzer only (LLM disabled or unavailable)")
    
    def _check_ollama(self) -> bool:
        """Check Ollama Availability"""
        try:
            import ollama
            models = ollama.list()
            model_names = [m['name'] for m in models.get('models', [])]
            
            if not any(self.model in name for name in model_names):
                print(f"âš ï¸ Model {self.model} not found. Run: ollama pull {self.model}")
                return False
            
            return True
        except ConnectionError:
            print(f"âš ï¸ Ollama connection failed: Connection refused")
            return False
        except Exception as e:
            print(f"âš ï¸ Ollama not available: {str(e)}")
            return False
    
    def analyze_sample(self, 
                      baseline_df: pd.DataFrame, 
                      sample_df: pd.DataFrame,
                      baseline_name: str,
                      sample_name: str,
                      image_path: Optional[str] = None) -> Dict:
        """
        Analyze FTIR sample using hybrid approach
        
        Process:
        1. Run fast numerical analysis (FTIRAnalyzer) - <1s, always succeeds
        2. Optionally enhance with LLM for better language - 5-15s, if available
        3. Return structured results with both numerical facts and summaries
        
        Args:
            baseline_df: Baseline spectrum DataFrame (columns: X, Y)
            sample_df: Sample spectrum DataFrame (columns: X, Y)
            baseline_name: Name of baseline file
            sample_name: Name of sample file
            image_path: Optional path to graph image (for LLM enhancement)
            
        Returns:
            Dictionary containing:
            - ftir_analysis: Complete numerical analysis results
            - human_summary: Readable summary (from FTIRAnalyzer or LLM)
            - llm_enhanced: Boolean indicating if LLM was used
            - analysis_time: Total time taken
        """
        start_time = time.time()
        
        print(f"\nğŸ” Analyzing {sample_name}...")
        
        # Step 1: Core numerical analysis (ALWAYS runs, <1s)
        try:
            baseline_wn, baseline_abs = load_csv_spectrum_from_df(baseline_df)
            sample_wn, sample_abs = load_csv_spectrum_from_df(sample_df)
            
            ftir_result = self.ftir_analyzer.analyze_complete(
                baseline_wn, baseline_abs,
                sample_wn, sample_abs,
                baseline_name, sample_name
            )
            
            ftir_time = time.time() - start_time
            print(f"âœ… Numerical analysis complete in {ftir_time:.2f}s")
            
        except Exception as e:
            print(f"âŒ Numerical analysis failed: {str(e)}")
            return {
                'error': f"Analysis failed: {str(e)}",
                'ftir_analysis': None,
                'human_summary': f"Error analyzing {sample_name}: {str(e)}",
                'llm_enhanced': False,
                'analysis_time': time.time() - start_time
            }
        
        # Step 2: Optional LLM enhancement (only if available and image provided)
        llm_summary = None
        llm_enhanced = False
        
        if self.ollama_available and image_path and self.use_llm:
            try:
                llm_summary = self._enhance_with_llm(ftir_result, image_path, baseline_name, sample_name)
                llm_enhanced = True
                llm_time = time.time() - start_time - ftir_time
                print(f"âœ… LLM enhancement complete in {llm_time:.2f}s")
            except Exception as e:
                print(f"âš ï¸ LLM enhancement failed: {str(e)}, using fallback")
                llm_enhanced = False
        
        # Step 3: Choose best summary
        final_summary = llm_summary if llm_enhanced else ftir_result['human_summary']
        
        total_time = time.time() - start_time
        
        return {
            'ftir_analysis': ftir_result,
            'human_summary': final_summary,
            'llm_enhanced': llm_enhanced,
            'analysis_time': total_time,
            'recommendation': ftir_result['recommendation'],
            'peak_matches': ftir_result['peak_matches'],
            'critical_regions': ftir_result['critical_regions']
        }
    
    def _enhance_with_llm(self, ftir_result: Dict, image_path: str, 
                          baseline_name: str, sample_name: str) -> str:
        """
        Enhance FTIR analysis with LLM natural language
        
        Uses structured numerical facts from FTIRAnalyzer and asks LLM
        to create a professional maintenance-focused summary.
        """
        import ollama
        
        # Read and encode image
        with open(image_path, 'rb') as img_file:
            image_data = base64.b64encode(img_file.read()).decode('utf-8')
        
        # Create prompt with structured facts
        prompt = self._create_llm_enhancement_prompt(ftir_result, baseline_name, sample_name)
        
        # Call LLaVA
        response = ollama.chat(
            model=self.model,
            messages=[{
                'role': 'user',
                'content': prompt,
                'images': [image_data]
            }],
            options={
                'temperature': 0.1,
                'num_predict': 800,
                'num_ctx': 2048,
            }
        )
        
        return response['message']['content']
    
    def _create_llm_enhancement_prompt(self, ftir_result: Dict, 
                                       baseline_name: str, sample_name: str) -> str:
        """Create LLM prompt with structured facts"""
        
        rec = ftir_result['recommendation']
        peaks = ftir_result['peak_matches']
        regions = ftir_result['critical_regions']
        
        prompt = f"""You are an expert FTIR spectroscopy analyst. Write a concise 3-4 sentence maintenance summary based on these ACCURATE NUMERICAL FACTS:

â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
NUMERICAL ANALYSIS RESULTS (Use these exact values):
â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

Sample: {sample_name}
Baseline: {baseline_name}

OXIDATION ANALYSIS (1650-1800 cmâ»Â¹):
- Baseline max: {regions['baseline']['oxidation']['max']:.3f} A
- Sample max: {regions['sample']['oxidation']['max']:.3f} A
- Increase: {rec['oxidation_increase_pct']:+.1f}%

WATER CONTAMINATION (3200-3600 cmâ»Â¹):
- Sample max: {regions['sample']['water']['max']:.3f} A
- Severity: {rec['water_severity']}

PEAK CHANGES:
- New peaks detected: {peaks['n_new']}
- Peaks shifted: {len(peaks['shifts'])}
- Matched peaks: {peaks['n_matched']}

RECOMMENDATION:
- Action: {rec['action'].upper()}
- Confidence: {rec['confidence']:.0%}
- Retest interval: {rec['retest_interval']}

â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
GRAPH REFERENCE:
- GREEN LINE: Baseline ({baseline_name})
- BLUE LINE: Sample ({sample_name})

Write a professional assessment for a maintenance manager:
1. State the overall condition (use status emoji: âœ…ğŸŸ¡âš ï¸ğŸ”´)
2. Explain key findings with specific numbers
3. State the recommended action with timeline
4. Keep it concise and actionable (3-4 sentences)

Use ONLY the numerical values provided above. The graph is for visual context only."""
        
        return prompt
    
    def analyze_ftir_hybrid(self,
                            image_path: str,
                            baseline_name: str,
                            sample_name: str,
                            peak_analysis_data: str = "") -> str:
        """
        Legacy method for backward compatibility
        
        This method wraps the new analyze_sample() method but returns
        only the human summary string (matching old signature).
        
        DEPRECATED: Use analyze_sample() for full structured results.
        """
        # Extract DataFrames from peak_analysis_data if needed
        # For now, we'll just use the FTIRAnalyzer-only mode
        result = {
            'human_summary': f"âš ï¸ Legacy method called. Please use analyze_sample() for full analysis.",
            'llm_enhanced': False
        }
        
        return result['human_summary']
    
    def analyze_samples_batch(self,
                             baseline_df: pd.DataFrame,
                             baseline_name: str,
                             samples: List[Tuple[pd.DataFrame, str]],
                             graph_paths: List[str] = None) -> Dict[str, Dict]:
        """
        Batch analyze multiple samples against one baseline
        
        Args:
            baseline_df: Baseline spectrum DataFrame
            baseline_name: Name of baseline file
            samples: List of (sample_df, sample_name) tuples
            graph_paths: Optional list of graph image paths (same order as samples)
            
        Returns:
            Dictionary mapping sample names to analysis results
        """
        results = {}
        total = len(samples)
        
        print(f"\n{'='*70}")
        print(f"BATCH ANALYSIS: {total} samples")
        print(f"{'='*70}")
        
        for idx, (sample_df, sample_name) in enumerate(samples, 1):
            print(f"\n[{idx}/{total}] Processing {sample_name}...")
            
            image_path = graph_paths[idx-1] if graph_paths and idx-1 < len(graph_paths) else None
            
            result = self.analyze_sample(
                baseline_df, 
                sample_df,
                baseline_name,
                sample_name,
                image_path
            )
            
            results[sample_name] = result
        
        print(f"\n{'='*70}")
        print(f"âœ… Batch analysis complete!")
        print(f"{'='*70}\n")
        
        return results
    
    def generate_summary(self, all_analyses: Dict[str, str]) -> str:
        """
        Generate Executive Summary
        """
        total = len(all_analyses)
        
        # Count action levels from analyses
        immediate = sum(1 for a in all_analyses.values() if 'IMMEDIATE' in a.upper())
        poor = sum(1 for a in all_analyses.values() if 'POOR' in a.upper() or 'CRITICAL' in a.upper())
        
        summary = f"""
â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
EXECUTIVE SUMMARY
â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

Total Samples Analyzed: {total}

Status Overview:
- Samples requiring immediate action: {immediate}
- Samples in poor/critical condition: {poor}
- Samples acceptable: {total - poor}

Recommendation:
"""
        
        if immediate > 0:
            summary += f"âš ï¸ URGENT: {immediate} sample(s) need immediate replacement.\n"
        elif poor > 0:
            summary += f"âš ï¸ ATTENTION: {poor} sample(s) showing degradation - plan replacement.\n"
        else:
            summary += "âœ… All samples within acceptable parameters. Continue normal monitoring.\n"
        
        summary += "\nReview individual sample analyses below for detailed assessments.\n"
        summary += "â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n"
        
        return summary
    
    def chat_with_context(self, user_message: str, context: Dict) -> str:
        """
        Interactive Chat with Context
        
        Allows users to ask questions about their analysis results.
        The LLM has access to all analysis data and can provide insights.
        
        Args:
            user_message: User's question
            context: Dictionary containing analysis context
            
        Returns:
            AI assistant's response
        """
        if not self.ollama_available:
            return (
                "âš ï¸ Chat unavailable - Ollama not connected.\n\n"
                f"Please ensure Ollama is running and {self.model} is available."
            )
        
        try:
            import ollama
            
            # Build comprehensive context prompt
            context_prompt = self._build_chat_context_prompt(context)
            
            # Combine context with user message
            full_prompt = f"""{context_prompt}

USER QUESTION:
{user_message}

Please provide a helpful, concise answer based on the analysis data above. Be specific and reference actual data points when relevant."""
            
            # Call LLM (text-only, no image needed for chat)
            response = ollama.chat(
                model=self.model.replace(':7b-v1.6', ''),  # Use base model for chat
                messages=[{
                    'role': 'user',
                    'content': full_prompt
                }],
                options={
                    'temperature': 0.3,      # Slightly higher for conversational tone
                    'num_predict': 500,      # Shorter responses for chat
                    'num_ctx': 4096,         # Large context for all data
                }
            )
            
            return response['message']['content']
            
        except Exception as e:
            return f"âŒ Chat error: {str(e)}"
    
    def _build_chat_context_prompt(self, context: Dict) -> str:
        """Build context prompt from analysis data"""
        
        prompt = f"""You are an expert FTIR spectroscopy analyst helping a user understand their grease analysis results.

ANALYSIS CONTEXT:
â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

Baseline: {context.get('baseline_name', 'N/A')}
Total Samples: {context.get('sample_count', 0)}
Samples: {', '.join(context.get('sample_names', []))}
Currently Viewing: {context.get('current_sample', 'N/A')}

EXECUTIVE SUMMARY:
{context.get('analysis_summary', 'No summary available')}

"""
        
        # Add individual analyses
        individual = context.get('individual_analyses', {})
        if individual:
            prompt += "\nINDIVIDUAL SAMPLE ANALYSES:\n"
            prompt += "â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n"
            for sample_name, analysis in individual.items():
                prompt += f"\n{sample_name}:\n{analysis[:500]}...\n"  # Truncate for context size
        
        # Add current sample stats
        current_stats = context.get('current_sample_stats', {})
        if current_stats:
            prompt += f"\nCURRENT SAMPLE STATISTICS:\n"
            prompt += f"Quality Score: {current_stats.get('quality_score', 'N/A')}/100\n"
            prompt += f"Mean Deviation: {current_stats.get('mean_deviation', 'N/A')}%\n"
            prompt += f"Correlation: {current_stats.get('correlation', 'N/A')}\n"
        
        # Add recent chat history
        chat_history = context.get('chat_history', [])
        if chat_history:
            prompt += "\nRECENT CONVERSATION:\n"
            for msg in chat_history[-3:]:  # Last 3 messages
                role = "User" if msg['role'] == 'user' else "Assistant"
                prompt += f"{role}: {msg['content'][:200]}\n"
        
        prompt += "\nâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n"
        
        return prompt


# ============================================================================
# TEST CODE
# ============================================================================
if __name__ == "__main__":
    print("âœ… LLM Analyzer (Hybrid) Test")
    
    # Use the specific model we pulled
    analyzer = LLMAnalyzer(model="llava:7b-v1.6")
    
    # Test with a sample image path
    test_image = "test_graph.png"
    
    if analyzer.ollama_available:
        print(f"\nâœ… Ready for hybrid FTIR analysis with {analyzer.model}")
    else:
        print("\nâš ï¸ Ollama not available - install and pull model")
    
    print("\nâœ… Module loaded successfully")